{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "84FGT_D2iDmG"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBbnYGnIljL6"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "student: Thomas Reolon\n",
        "\n",
        "repository: [github](https://github.com/thomasreolon/NLU_unitn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJBB4jKTiDmF"
      },
      "source": [
        "## Assignment: Working with Dependency Graphs (Parses)\n",
        "\n",
        "The objective of the assignment is to learn how to work with dependency graphs by defining functions.\n",
        "\n",
        "Read [spaCy documentation on dependency parser](https://spacy.io/api/dependencyparser) to learn provided methods.\n",
        "\n",
        "Define functions to:\n",
        "- extract a path of dependency relations from the ROOT to a token\n",
        "- extract subtree of a dependents given a token\n",
        "- check if a given list of tokens (segment of a sentence) forms a subtree\n",
        "- identify head of a span, given its tokens\n",
        "- extract sentence subject, direct object and indirect object spans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7QcFdbsOBih"
      },
      "source": [
        "**Setting up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQi3vycgoJXT"
      },
      "source": [
        "!python -m spacy download en_core_web_md | cat > hide_cell_output.txt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUr1IlulmLtg"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_md\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "#parser = nlp.get_pipe(\"parser\") # component of nlp that create dep_graph"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g7mjbnGmaRP"
      },
      "source": [
        "##### **1. extract a path of dependency relations from the ROOT to a token**\n",
        "\n",
        "> get_path_from_root\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrYzT_RMiDmF"
      },
      "source": [
        "def get_path_from_root(sentence:str):\n",
        "  \"\"\"returns a dict {token:list_of_dependencies} for every torken in the sentence\"\"\"\n",
        "  doc = nlp(sentence)\n",
        "  paths = {}\n",
        "  for token in doc:\n",
        "    tmp = token\n",
        "    path = []\n",
        "    while tmp != tmp.head:              # while we are not at root\n",
        "      path.append(tmp.dep_)             # append dependency to path (a dependency is an arc in the graph)\n",
        "      tmp = tmp.head                    # move to parent node\n",
        "    path.append('ROOT')\n",
        "    paths[token] = list(reversed(path)) # path starts from ROOT\n",
        "  return paths"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0JGMePntntB",
        "outputId": "6b43c15a-4383-4ec2-9b54-ebe55fd9bd7b"
      },
      "source": [
        "# small test\n",
        "\n",
        "sentences = [\n",
        "  'Mary was having breakfast two minutes ago.',\n",
        "  'You can watch the movie while cooking',\n",
        "  'I saw a man with a telescope.'\n",
        "]\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  print('\\n----------', sent, '----------')\n",
        "  all_paths = get_path_from_root(sent)        # process sentence\n",
        "  for tok, path in all_paths.items():\n",
        "    print(f'token:{tok.text+\" \"*(10-len(tok))}: {path}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Mary was having breakfast two minutes ago. ----------\n",
            "token:Mary      : ['ROOT', 'nsubj']\n",
            "token:was       : ['ROOT', 'aux']\n",
            "token:having    : ['ROOT']\n",
            "token:breakfast : ['ROOT', 'dobj']\n",
            "token:two       : ['ROOT', 'advmod', 'npadvmod', 'nummod']\n",
            "token:minutes   : ['ROOT', 'advmod', 'npadvmod']\n",
            "token:ago       : ['ROOT', 'advmod']\n",
            "token:.         : ['ROOT', 'punct']\n",
            "\n",
            "---------- You can watch the movie while cooking ----------\n",
            "token:You       : ['ROOT', 'nsubj']\n",
            "token:can       : ['ROOT', 'aux']\n",
            "token:watch     : ['ROOT']\n",
            "token:the       : ['ROOT', 'dobj', 'det']\n",
            "token:movie     : ['ROOT', 'dobj']\n",
            "token:while     : ['ROOT', 'advcl', 'mark']\n",
            "token:cooking   : ['ROOT', 'advcl']\n",
            "\n",
            "---------- I saw a man with a telescope. ----------\n",
            "token:I         : ['ROOT', 'nsubj']\n",
            "token:saw       : ['ROOT']\n",
            "token:a         : ['ROOT', 'dobj', 'det']\n",
            "token:man       : ['ROOT', 'dobj']\n",
            "token:with      : ['ROOT', 'dobj', 'prep']\n",
            "token:a         : ['ROOT', 'dobj', 'prep', 'pobj', 'det']\n",
            "token:telescope : ['ROOT', 'dobj', 'prep', 'pobj']\n",
            "token:.         : ['ROOT', 'punct']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNkATeN6oES9"
      },
      "source": [
        "##### **2. extract subtree of a dependents given a token**\n",
        "\n",
        "> get_subtree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5gFW30_wkjn"
      },
      "source": [
        "def get_subtree(sentence:str):\n",
        "  \"\"\"returns a dict {token:subtree(token)} for every token in the sentence\"\"\"\n",
        "  doc = nlp(sentence)\n",
        "  subtrees = {}\n",
        "  for token in doc:\n",
        "    subtrees[token] = [t.text for t in token.subtree] # iterate through subtree generator and extract tokens\n",
        "  return subtrees"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV7U8wy3uY2J",
        "outputId": "14ff71c1-009f-4049-faf3-b1e325bbadb8"
      },
      "source": [
        "# small test\n",
        "\n",
        "for sent in sentences:\n",
        "  print('\\n----------', sent, '----------')\n",
        "  subtrees = get_subtree(sent)\n",
        "  for tok, st in subtrees.items():\n",
        "    print(f'token:{tok.text+\" \"*(10-len(tok))} subtree: {st}')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Mary was having breakfast two minutes ago. ----------\n",
            "token:Mary       subtree: ['Mary']\n",
            "token:was        subtree: ['was']\n",
            "token:having     subtree: ['Mary', 'was', 'having', 'breakfast', 'two', 'minutes', 'ago', '.']\n",
            "token:breakfast  subtree: ['breakfast']\n",
            "token:two        subtree: ['two']\n",
            "token:minutes    subtree: ['two', 'minutes']\n",
            "token:ago        subtree: ['two', 'minutes', 'ago']\n",
            "token:.          subtree: ['.']\n",
            "\n",
            "---------- You can watch the movie while cooking ----------\n",
            "token:You        subtree: ['You']\n",
            "token:can        subtree: ['can']\n",
            "token:watch      subtree: ['You', 'can', 'watch', 'the', 'movie', 'while', 'cooking']\n",
            "token:the        subtree: ['the']\n",
            "token:movie      subtree: ['the', 'movie']\n",
            "token:while      subtree: ['while']\n",
            "token:cooking    subtree: ['while', 'cooking']\n",
            "\n",
            "---------- I saw a man with a telescope. ----------\n",
            "token:I          subtree: ['I']\n",
            "token:saw        subtree: ['I', 'saw', 'a', 'man', 'with', 'a', 'telescope', '.']\n",
            "token:a          subtree: ['a']\n",
            "token:man        subtree: ['a', 'man', 'with', 'a', 'telescope']\n",
            "token:with       subtree: ['with', 'a', 'telescope']\n",
            "token:a          subtree: ['a']\n",
            "token:telescope  subtree: ['a', 'telescope']\n",
            "token:.          subtree: ['.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGYaQhwGoclG"
      },
      "source": [
        "##### **3. check if a given list of tokens (segment of a sentence) forms a subtree**\n",
        "\n",
        "> is_subtree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q89ZnI1_xR7I"
      },
      "source": [
        "def is_subtree(sentence:str, list_of_tokens:list):\n",
        "  \"\"\"check if a list of tokens is a subtree\"\"\"\n",
        "  if len(list_of_tokens)==0: return True                            # default answer for empty graph (True)\n",
        "  input_subtree = [str(t) for t in list_of_tokens]                  # in case list of token was a Span instance or a Doc\n",
        "  possible_subtrees = [s for _,s in get_subtree(sentence).items()]  # get all possible subtree using function 2\n",
        "  return input_subtree in possible_subtrees                         # if list_of_tokens is a valid subtree it must be in the possible subtrees"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqlzxh6zvlvB",
        "outputId": "74792edf-3129-44a1-e742-7babd46cf944"
      },
      "source": [
        "# small test\n",
        "\n",
        "doc = nlp(sent)\n",
        "print(is_subtree('I saw a man with a telescope.', ['with', 'a', 'telescope']))\n",
        "print(is_subtree('I saw a man with a telescope.', ['with', 'saw', 'telescope']))\n",
        "\n",
        "print(is_subtree(  sentences[0], nlp(sentences[0])[-4:-1]  ))  # Mary was having breakfast two minutes ago.   [ two, minutes, ago ]\n",
        "print(is_subtree(  sentences[0], nlp(sentences[0])[-4:]  ))    # Mary was having breakfast two minutes ago.   [ two, minutes, ago, . ]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDSiTLgmpCEU"
      },
      "source": [
        "##### **4. identify head of a span, given its tokens**\n",
        "\n",
        "> get_head_of_span"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rrg_oTB4XFb"
      },
      "source": [
        "def get_head_of_span(span):\n",
        "  \"\"\"return the head node of a span, as a Token object, can handle multiple inputs\"\"\"\n",
        "  if isinstance(span, spacy.tokens.Span):     # we can extract the root of a span with the 'root' attribute\n",
        "    return span.root\n",
        "  elif isinstance(span, spacy.tokens.Doc):    # if we have a doc, doc[:] returns a span of the whole sentence\n",
        "    return span[:].root\n",
        "  elif isinstance(span, list):                # if we have a list of tokens (eg. ['i', 'like', 'pizza']) we can convert it into a string and process it using spacy\n",
        "    return nlp(' '.join(span))[:].root\n",
        "  elif isinstance(span, str):                 # if we have a string we can obtain a Doc using nlp, then get the span with [:], then return root\n",
        "    return nlp(span)[:].root"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwlxegpxxLjp",
        "outputId": "f51e1f70-36db-4b8f-89cb-8eb6c85be9cd"
      },
      "source": [
        "# small test\n",
        "\n",
        "span = sentences[0].split()[-4:]\n",
        "print(span, '---->', get_head_of_span(span))\n",
        "\n",
        "span = nlp(sentences[1])[:4]\n",
        "print(span, '---->', get_head_of_span(span))\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['breakfast', 'two', 'minutes', 'ago.'] ----> breakfast\n",
            "You can watch the ----> watch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuLiu5IbpSiP"
      },
      "source": [
        "##### **5. extract sentence subject, direct object and indirect object spans**\n",
        "\n",
        "> extract_sub_ind_dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb4ukS5G5NF4"
      },
      "source": [
        "def extract_sub_ind_dir(sentence:str):\n",
        "  \"\"\"returns subject, indirect obj, direct obj of a sentence\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  sentence:          str\n",
        "    the sentence to process, eg. 'I saw Luca.'\n",
        "\n",
        "  returns a dict of lists\n",
        "  \"\"\"\n",
        "  out = {'SUBJECT':[], 'DIRECT-OBJECT':[], 'INDIRECT-OBJECT':[]}\n",
        "  subjs = {'nsubjpass', 'csubj', 'expl', 'csubjpass'}\n",
        "  doc = nlp(sentence)\n",
        "  for token in doc:\n",
        "    if   token.dep_=='dative' : out['INDIRECT-OBJECT'] = [str(t) for t in token.subtree]\n",
        "    elif token.dep_=='dobj':    out['DIRECT-OBJECT'] = [str(t) for t in token.subtree]\n",
        "    elif token.dep_ in subjs:   out['SUBJECT'] = [str(t) for t in token.subtree]\n",
        "    elif token.dep_=='nsubj' and len(out['SUBJECT'])==0:\n",
        "      # sometimes there is a token with a nsubj dependency and another one ( eg. csubj )\n",
        "      # we give priority to the others over nsubj\n",
        "      # eg. What you did was unforgivable --> nsubj=you,  csubj=what you did \n",
        "      out['SUBJECT'] = [str(t) for t in token.subtree]  \n",
        "\n",
        "  return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWFQtE5uyobx",
        "outputId": "5bf6341b-488f-4b96-be10-dd810849b648"
      },
      "source": [
        "# small test\n",
        "\n",
        "# 'her' is an indirect object, which, in spaCy, is identified by 'dative'  ((even if stack overflow said 'iobj'))\n",
        "s = 'Please give her a copy of the article by tomorrow morning'\n",
        "print(s, '->', extract_sub_ind_dir(s))\n",
        "\n",
        "print('')\n",
        "for sent in sentences:\n",
        "  print(sent, '-->', extract_sub_ind_dir(sent))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please give her a copy of the article by tomorrow morning -> {'SUBJECT': [], 'DIRECT-OBJECT': ['a', 'copy', 'of', 'the', 'article'], 'INDIRECT-OBJECT': ['her']}\n",
            "\n",
            "Mary was having breakfast two minutes ago. --> {'SUBJECT': ['Mary'], 'DIRECT-OBJECT': ['breakfast'], 'INDIRECT-OBJECT': []}\n",
            "You can watch the movie while cooking --> {'SUBJECT': ['You'], 'DIRECT-OBJECT': ['the', 'movie'], 'INDIRECT-OBJECT': []}\n",
            "I saw a man with a telescope. --> {'SUBJECT': ['I'], 'DIRECT-OBJECT': ['a', 'man', 'with', 'a', 'telescope'], 'INDIRECT-OBJECT': []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfkbjmvlY0np",
        "outputId": "da1dbd39-2f03-480c-d100-ec3ff5b4f14a"
      },
      "source": [
        "# test subjects\n",
        "sents = [\n",
        "  'I like pizza.',  #  I\n",
        "  'Mary is cutting a carrot',  #  Mary\n",
        "  'A carrot is being cut by Mary.',  #  A carrot\n",
        "  'Obviously, the new design has a greater user interface',  #  the new design  (expl: Obviously)\n",
        "  'What you did was unforgivable',  # What you did\n",
        "  'What you did was not appreciated by the others',   # What you did\n",
        "  'There is only one car in the park'  # There\n",
        "]\n",
        "\n",
        "for sent in sents:\n",
        "  print(sent, '-->', extract_sub_ind_dir(sent))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like pizza. --> {'SUBJECT': ['I'], 'DIRECT-OBJECT': ['pizza'], 'INDIRECT-OBJECT': []}\n",
            "Mary is cutting a carrot --> {'SUBJECT': ['Mary'], 'DIRECT-OBJECT': ['a', 'carrot'], 'INDIRECT-OBJECT': []}\n",
            "A carrot is being cut by Mary. --> {'SUBJECT': ['A', 'carrot'], 'DIRECT-OBJECT': [], 'INDIRECT-OBJECT': []}\n",
            "Obviously, the new design has a greater user interface --> {'SUBJECT': ['the', 'new', 'design'], 'DIRECT-OBJECT': ['a', 'greater', 'user', 'interface'], 'INDIRECT-OBJECT': []}\n",
            "What you did was unforgivable --> {'SUBJECT': ['What', 'you', 'did'], 'DIRECT-OBJECT': ['What'], 'INDIRECT-OBJECT': []}\n",
            "What you did was not appreciated by the others --> {'SUBJECT': ['What', 'you', 'did'], 'DIRECT-OBJECT': ['What'], 'INDIRECT-OBJECT': []}\n",
            "There is only one car in the park --> {'SUBJECT': ['There'], 'DIRECT-OBJECT': [], 'INDIRECT-OBJECT': []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FGT_D2iDmG"
      },
      "source": [
        "## Assignment: Training Transition-Based Dependency Parser (Optional & Advanced)\n",
        "\n",
        "- Modify [NLTK Transition parser](https://github.com/nltk/nltk/blob/develop/nltk/parse/transitionparser.py)'s `Configuration` class to use better features.\n",
        "- Evaluate the features comparing performance to the original\n",
        "- Replace `SVM` classifier with an alternative of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOaBJkiISUE"
      },
      "source": [
        "from nltk.parse.transitionparser import *  # contains Configuration, TransitionParser and more"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zrh7yIMIyQf"
      },
      "source": [
        "# from: https://github.com/nltk/nltk/blob/develop/nltk/parse/transitionparser.py\n",
        "# with some changes to:\n",
        "#  --> config: \n",
        "#         removed lemma:    noticeable impvrovment (probably redundant infos with 'word' argument)\n",
        "#         word.lower():     did not impove accuracy, but i kept it\n",
        "#         add dependencies: little improvement\n",
        "#\n",
        "#  --> model: SVM classifier kernel change from POLINOMIAL to RBF (and some hyperparameters)\n",
        "#         incrementing C:   noticeable improvement (probably the boundary feature space is not regular)\n",
        "#         decreasing gamma: little improvement (remove this change if you are training on the whole treebank dataset (if you have many training points you can have an higher gamma))\n",
        "\n",
        "class MyConfiguration(Configuration):\n",
        "  def __init__(self, dep_graph):\n",
        "    super().__init__(dep_graph)\n",
        "\n",
        "  def extract_features(self):\n",
        "    result = []\n",
        "    if len(self.stack) > 0:\n",
        "      # Stack 0\n",
        "      stack_idx0 = self.stack[len(self.stack) - 1]\n",
        "      token = self._tokens[stack_idx0]\n",
        "      if self._check_informative(token[\"word\"], True):\n",
        "        result.append(\"STK_0_FORM_\" + token[\"word\"].lower())\n",
        "      if self._check_informative(token[\"tag\"]):\n",
        "        result.append(\"STK_0_POS_\" + token[\"tag\"])\n",
        "      if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
        "        feats = token[\"feats\"].split(\"|\")\n",
        "        for feat in feats:\n",
        "          result.append(\"STK_0_FEATS_\" + feat)\n",
        "      # Stack 1\n",
        "      if len(self.stack) > 1:\n",
        "        stack_idx1 = self.stack[len(self.stack) - 2]\n",
        "        token = self._tokens[stack_idx1]\n",
        "        if self._check_informative(token[\"tag\"]):\n",
        "          result.append(\"STK_1_POS_\" + token[\"tag\"])\n",
        "\n",
        "      # Left most, right most dependency of stack[0]\n",
        "      left_most = 1000000\n",
        "      right_most = -1\n",
        "      dep_left_most = \"\"\n",
        "      dep_right_most = \"\"\n",
        "      for (wi, r, wj) in self.arcs:\n",
        "        if wi == stack_idx0:\n",
        "          if (wj > wi) and (wj > right_most):\n",
        "            right_most = wj\n",
        "            dep_right_most = r\n",
        "          if (wj < wi) and (wj < left_most):\n",
        "            left_most = wj\n",
        "            dep_left_most = r\n",
        "      if self._check_informative(dep_left_most):\n",
        "        result.append(\"STK_0_LDEP_\" + dep_left_most)\n",
        "      if self._check_informative(dep_right_most):\n",
        "        result.append(\"STK_0_RDEP_\" + dep_right_most)\n",
        "\n",
        "    # Check Buffered 0\n",
        "    if len(self.buffer) > 0:\n",
        "      # Buffer 0\n",
        "      buffer_idx0 = self.buffer[0]\n",
        "      token = self._tokens[buffer_idx0]\n",
        "      if self._check_informative(token[\"word\"], True):\n",
        "          result.append(\"BUF_0_FORM_\" + token[\"word\"].lower())\n",
        "      if self._check_informative(token[\"tag\"]):\n",
        "          result.append(\"BUF_0_POS_\" + token[\"tag\"])\n",
        "      if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
        "          feats = token[\"feats\"].split(\"|\")\n",
        "          for feat in feats:\n",
        "              result.append(\"BUF_0_FEATS_\" + feat)\n",
        "      if \"deps\" in token and len(token[\"deps\"])>0:\n",
        "        for d in token[\"deps\"]:\n",
        "          result.append(\"DEP_0_\"+str(d))\n",
        "      #if \"head\" in token and token['head']:\n",
        "      #  result.append(\"HEAD_0_\"+str(token['head']))\n",
        "\n",
        "      \n",
        "      # Buffer 1\n",
        "      if len(self.buffer) > 1:\n",
        "        buffer_idx1 = self.buffer[1]\n",
        "        token = self._tokens[buffer_idx1]\n",
        "        if self._check_informative(token[\"word\"], True):\n",
        "          result.append(\"BUF_1_FORM_\" + token[\"word\"])\n",
        "        if self._check_informative(token[\"tag\"]):\n",
        "          result.append(\"BUF_1_POS_\" + token[\"tag\"])\n",
        "        if \"deps\" in token and len(token[\"deps\"])>0:\n",
        "          for d in token[\"deps\"]:\n",
        "            result.append(\"DEP_1_\"+str(d))\n",
        "      if len(self.buffer) > 2:\n",
        "        buffer_idx2 = self.buffer[2]\n",
        "        token = self._tokens[buffer_idx2]\n",
        "        if self._check_informative(token[\"tag\"]):\n",
        "          result.append(\"BUF_2_POS_\" + token[\"tag\"])\n",
        "      if len(self.buffer) > 3:\n",
        "        buffer_idx3 = self.buffer[3]\n",
        "        token = self._tokens[buffer_idx3]\n",
        "        if self._check_informative(token[\"tag\"]):\n",
        "          result.append(\"BUF_3_POS_\" + token[\"tag\"])\n",
        "          # Left most, right most dependency of stack[0]\n",
        "      left_most = 1000000\n",
        "      right_most = -1\n",
        "      dep_left_most = \"\"\n",
        "      dep_right_most = \"\"\n",
        "      for (wi, r, wj) in self.arcs:\n",
        "        if wi == buffer_idx0:\n",
        "          if (wj > wi) and (wj > right_most):\n",
        "            right_most = wj\n",
        "            dep_right_most = r\n",
        "          if (wj < wi) and (wj < left_most):\n",
        "            left_most = wj\n",
        "            dep_left_most = r\n",
        "      if self._check_informative(dep_left_most):\n",
        "        result.append(\"BUF_0_LDEP_\" + dep_left_most)\n",
        "      if self._check_informative(dep_right_most):\n",
        "        result.append(\"BUF_0_RDEP_\" + dep_right_most)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfN_jrsKJsRx"
      },
      "source": [
        "class MyParser(TransitionParser):\n",
        "  def __init__(self):\n",
        "    super().__init__(\"arc-eager\")\n",
        "\n",
        "  def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
        "    operation = Transition(self.ARC_EAGER)\n",
        "    countProj = 0\n",
        "    training_seq = []\n",
        "\n",
        "    for depgraph in depgraphs:\n",
        "      if not self._is_projective(depgraph):\n",
        "        continue\n",
        "\n",
        "      countProj += 1\n",
        "      conf = MyConfiguration(depgraph)\n",
        "      while len(conf.buffer) > 0:\n",
        "        b0 = conf.buffer[0]\n",
        "        features = conf.extract_features()\n",
        "        binary_features = self._convert_to_binary_features(features)\n",
        "\n",
        "        if len(conf.stack) > 0:\n",
        "          s0 = conf.stack[len(conf.stack) - 1]\n",
        "          # Left-arc operation\n",
        "          rel = self._get_dep_relation(b0, s0, depgraph)\n",
        "          if rel is not None:\n",
        "            key = Transition.LEFT_ARC + \":\" + rel\n",
        "            self._write_to_file(key, binary_features, input_file)\n",
        "            operation.left_arc(conf, rel)\n",
        "            training_seq.append(key)\n",
        "            continue\n",
        "\n",
        "          # Right-arc operation\n",
        "          rel = self._get_dep_relation(s0, b0, depgraph)\n",
        "          if rel is not None:\n",
        "            key = Transition.RIGHT_ARC + \":\" + rel\n",
        "            self._write_to_file(key, binary_features, input_file)\n",
        "            operation.right_arc(conf, rel)\n",
        "            training_seq.append(key)\n",
        "            continue\n",
        "\n",
        "          # reduce operation\n",
        "          flag = False\n",
        "          for k in range(s0):\n",
        "            if self._get_dep_relation(k, b0, depgraph) is not None:\n",
        "              flag = True\n",
        "            if self._get_dep_relation(b0, k, depgraph) is not None:\n",
        "              flag = True\n",
        "          if flag:\n",
        "            key = Transition.REDUCE\n",
        "            self._write_to_file(key, binary_features, input_file)\n",
        "            operation.reduce(conf)\n",
        "            training_seq.append(key)\n",
        "            continue\n",
        "\n",
        "        # Shift operation as the default\n",
        "        key = Transition.SHIFT\n",
        "        self._write_to_file(key, binary_features, input_file)\n",
        "        operation.shift(conf)\n",
        "        training_seq.append(key)\n",
        "\n",
        "    return training_seq\n",
        "\n",
        "  def train(self, depgraphs, modelfile, verbose=True):\n",
        "    try:\n",
        "      input_file = tempfile.NamedTemporaryFile(\n",
        "        prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
        "      )\n",
        "\n",
        "      if self._algorithm == self.ARC_STANDARD:\n",
        "        self._create_training_examples_arc_std(depgraphs, input_file)\n",
        "      else:\n",
        "        self._create_training_examples_arc_eager(depgraphs, input_file)\n",
        "\n",
        "      input_file.close()\n",
        "      x_train, y_train = load_svmlight_file(input_file.name)\n",
        "      model = svm.SVC(\n",
        "          kernel=\"rbf\",\n",
        "          coef0=0,\n",
        "          gamma=0.08, # ---> low = higher variance (easier similarity)\n",
        "          C=4,        # ---> high = high penalization for errors\n",
        "          verbose=verbose,\n",
        "          probability=True,\n",
        "      )\n",
        "\n",
        "      model.fit(x_train, y_train)\n",
        "      pickle.dump(model, open(modelfile, \"wb\"))\n",
        "    finally:\n",
        "      remove(input_file.name)\n",
        "  \n",
        "  def parse(self, depgraphs, modelFile):\n",
        "    result = []\n",
        "    # First load the model\n",
        "    model = pickle.load(open(modelFile, \"rb\"))\n",
        "    operation = Transition(self._algorithm)\n",
        "\n",
        "    for depgraph in depgraphs:\n",
        "      conf = MyConfiguration(depgraph)\n",
        "      while len(conf.buffer) > 0:\n",
        "        features = conf.extract_features()\n",
        "        col = []\n",
        "        row = []\n",
        "        data = []\n",
        "        for feature in features:\n",
        "          if feature in self._dictionary:\n",
        "            col.append(self._dictionary[feature])\n",
        "            row.append(0)\n",
        "            data.append(1.0)\n",
        "        np_col = array(sorted(col))  # NB : index must be sorted\n",
        "        np_row = array(row)\n",
        "        np_data = array(data)\n",
        "\n",
        "        x_test = sparse.csr_matrix(\n",
        "            (np_data, (np_row, np_col)), shape=(1, len(self._dictionary))\n",
        "        )\n",
        "\n",
        "        prob_dict = {}\n",
        "        pred_prob = model.predict_proba(x_test)[0]\n",
        "        for i in range(len(pred_prob)):\n",
        "          prob_dict[i] = pred_prob[i]\n",
        "        sorted_Prob = sorted(prob_dict.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "        # Note that SHIFT is always a valid operation\n",
        "        for (y_pred_idx, confidence) in sorted_Prob:\n",
        "          y_pred = model.classes_[y_pred_idx]\n",
        "\n",
        "          if y_pred in self._match_transition:\n",
        "            strTransition = self._match_transition[y_pred]\n",
        "            baseTransition = strTransition.split(\":\")[0]\n",
        "\n",
        "            if baseTransition == Transition.LEFT_ARC:\n",
        "              if (\n",
        "                  operation.left_arc(conf, strTransition.split(\":\")[1])\n",
        "                  != -1\n",
        "              ):break\n",
        "            elif baseTransition == Transition.RIGHT_ARC:\n",
        "              if (\n",
        "                  operation.right_arc(conf, strTransition.split(\":\")[1])\n",
        "                  != -1\n",
        "              ):break\n",
        "            elif baseTransition == Transition.REDUCE:\n",
        "              if operation.reduce(conf) != -1:break\n",
        "            elif baseTransition == Transition.SHIFT:\n",
        "              if operation.shift(conf) != -1:break\n",
        "          else:\n",
        "            raise ValueError(\n",
        "                \"The predicted transition is not recognized, expected errors\"\n",
        "            )\n",
        "\n",
        "        # Finish with operations build the dependency graph from Conf.arcs\n",
        "\n",
        "        new_depgraph = deepcopy(depgraph)\n",
        "        for key in new_depgraph.nodes:\n",
        "            node = new_depgraph.nodes[key]\n",
        "            node[\"rel\"] = \"\"\n",
        "            # With the default, all the token depend on the Root\n",
        "            node[\"head\"] = 0\n",
        "        for (head, rel, child) in conf.arcs:\n",
        "            c_node = new_depgraph.nodes[child]\n",
        "            c_node[\"head\"] = head\n",
        "            c_node[\"rel\"] = rel\n",
        "      result.append(new_depgraph)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC65inwmKmCH",
        "outputId": "f28c2cf8-bc0c-416e-da7b-16a111783259"
      },
      "source": [
        "import nltk\n",
        "from nltk.parse import DependencyEvaluator\n",
        "nltk.download('dependency_treebank')\n",
        "from nltk.corpus import dependency_treebank"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package dependency_treebank to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/dependency_treebank.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXHrAE6BK6SN",
        "outputId": "9146d79d-4a99-425d-b686-57bf29b7bd85"
      },
      "source": [
        "# default NLTK parser, arc-standard\n",
        "\n",
        "train_size = 200\n",
        "test_size  = 50\n",
        "\n",
        "tp = TransitionParser('arc-standard')\n",
        "tp.train(dependency_treebank.parsed_sents()[:train_size], 'tp.model')\n",
        "parses = tp.parse(dependency_treebank.parsed_sents()[-test_size:], 'tp.model')\n",
        "\n",
        "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-test_size:])\n",
        "print(f'\\n\\nscores of basic Transition Parser from nltk = {de.eval()}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[LibSVM]\n",
            "\n",
            "scores of basic Transition Parser from nltk = (0.8152709359605911, 0.8152709359605911)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBg0xM8hLV9u",
        "outputId": "13ead945-2f3e-406a-dfa6-2b1c01a3ec38"
      },
      "source": [
        "# my random parser, arc-eager\n",
        "\n",
        "# train\n",
        "tp = MyParser()\n",
        "tp.train(dependency_treebank.parsed_sents()[:train_size], 'tp2.model')\n",
        "\n",
        "# test\n",
        "input = dependency_treebank.parsed_sents()[-test_size:]\n",
        "parses = tp.parse(input, 'tp2.model')\n",
        "print(len(input), len(parses))\n",
        "de = DependencyEvaluator(parses, input)\n",
        "print(f'\\n\\nscores of MyParser = {de.eval()}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]50 50\n",
            "\n",
            "\n",
            "scores of MyParser = (0.8555008210180624, 0.8555008210180624)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}