{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretrained Abstract Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7859a90e46cb4ba7bf8d204dae6f283f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3a828d26e5340fbbcd0a326276b0d19",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7e4abe48d1d41829f8858f5e7749da0",
              "IPY_MODEL_45a2ce75147346c591ff5d32b9cb2f35",
              "IPY_MODEL_8d0298aeb61c413183b48408ed2d4659"
            ]
          }
        },
        "f3a828d26e5340fbbcd0a326276b0d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7e4abe48d1d41829f8858f5e7749da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83b9a6ae36324bb9ba444217ec26298a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6c55ea1c0914d7f86d324ae47c38704"
          }
        },
        "45a2ce75147346c591ff5d32b9cb2f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09eab332aec14d8dbb2abb264a3b0bfc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2170,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2170,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba59fd811dbc4e308006f4ea640bed7b"
          }
        },
        "8d0298aeb61c413183b48408ed2d4659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bf910a9a8be4b3f959dcc14d30ed8e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.61k/? [00:00&lt;00:00, 130kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64ddd51cef4740ddb0bafdc4b63417c8"
          }
        },
        "83b9a6ae36324bb9ba444217ec26298a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6c55ea1c0914d7f86d324ae47c38704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09eab332aec14d8dbb2abb264a3b0bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba59fd811dbc4e308006f4ea640bed7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bf910a9a8be4b3f959dcc14d30ed8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64ddd51cef4740ddb0bafdc4b63417c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khFS_S5zLeR2"
      },
      "source": [
        "## Abstract Summarizaiton\n",
        "\n",
        "In this notebook we load a pretrained BART neural net with more than 200M parameters. Given the size of this network it would be impossible to train it on my own.\n",
        "\n",
        "This model was trained on Xsum dataset (extreme summarization) and our objective would be to finetune it. Unfortunately, the finetuning part do not work properly, I'd guess for one of these 2 reasons: the model is soo big that even finetuning it requires more computing power than what we have access to; it was trained with a totally different loss (wrt. the one used by huggingface), thus finetuning do not work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZEN69tFt2IK"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ooRPRtztA"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers.optimization import AdamW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWKC1A9HvMT0"
      },
      "source": [
        "encoder_max_length = 400\n",
        "decoder_max_length = 75\n",
        "batch_size         = 8\n",
        "model_name         = 'sshleifer/distilbart-cnn-12-3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T12Q31H6JaX9"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# just to download stuff at the beginning, hiding the logs after\n",
        "BartForConditionalGeneration.from_pretrained(model_name)\n",
        "load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "BartTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igbNiVv3v2aN"
      },
      "source": [
        "\"\"\"\n",
        "Uses https://huggingface.co/datasets/viewer/?dataset=cnn_dailymail and a tokenizer\n",
        "to create the hot-encodings to feed to the NN\n",
        "\"\"\"\n",
        "\n",
        "class CNNDataset(Dataset):\n",
        "  def __init__(self, mode='train', n_articles=10000):\n",
        "    super().__init__()\n",
        "    raw_data  = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=f\"{mode}[:{n_articles}]\")  # download dataset from huggingface\n",
        "    self.tokenizer = BartTokenizer.from_pretrained(model_name)    # load tokenizer to hot-encode articles\n",
        "    self.data = self.preprocess_data(raw_data)                                          # preprocess data (hot-encode article/highlights)\n",
        "\n",
        "  def preprocess_data(self, raw_data):\n",
        "    data = []\n",
        "    for i in range(len(raw_data)):\n",
        "      hot_article = self.tokenizer(raw_data[i]['article'], padding=\"max_length\", truncation=True, max_length=encoder_max_length, return_tensors=\"pt\")\n",
        "      with self.tokenizer.as_target_tokenizer():\n",
        "        hot_high  = self.tokenizer(raw_data[i]['highlights'], padding=\"max_length\", truncation=True, max_length=decoder_max_length, return_tensors=\"pt\")\n",
        "      labels      = torch.tensor([-100 if token == self.tokenizer.pad_token_id else token for token in hot_high.input_ids[0]], dtype=torch.long)\n",
        "      data.append( {\n",
        "          'input_ids':hot_article.input_ids[0],       # article\n",
        "          'decoder_input_ids':hot_high.input_ids[0],  # summary\n",
        "          'labels':labels,                            # summary (excluding <pad> from loss)\n",
        "          'attention_mask':hot_article.attention_mask[0],\n",
        "          'decoder_attention_mask':hot_high.attention_mask[0]\n",
        "      } )\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpGrKBBzMbnt",
        "outputId": "50471cba-6469-48b6-b610-2106c7c8d192"
      },
      "source": [
        "\"\"\"\n",
        "Instantiate model, dataloader & optimizer\n",
        "\"\"\"\n",
        "\n",
        "# model trained on xetreme summarization task\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name).cuda()\n",
        "\n",
        "# data\n",
        "train_data = CNNDataset(mode='train', n_articles=1000)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "# optimizer\n",
        "params = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\n",
        "        \"weight_decay\": 1e-8,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "optim = torch.optim.AdamW(params, lr=3e-8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hRsAg9l7660"
      },
      "source": [
        "## Finetuning not working\n",
        "\n",
        "--> I think that the model was pretrained with a particular loss, for this reason finetuning do not work properly (or we just lack computing power)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEa9-U7a9Eob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb9d57f-572c-45db-db11-b74411bccca7"
      },
      "source": [
        "# finetuning not working\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):\n",
        "  tot_loss = []\n",
        "  for data in tqdm(train_loader, postfix=True):\n",
        "\n",
        "    # move to GPU\n",
        "    for k in data:\n",
        "      data[k] = data[k].cuda()\n",
        "\n",
        "    # forward\n",
        "    loss = model(**data)[0]\n",
        "\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    tot_loss.append(loss.item())\n",
        "\n",
        "    # print stats\n",
        "    if len(tot_loss)%100==0: \n",
        "      print(f'loss e:{epoch} = {1000*sum(tot_loss)/len(tot_loss):.5f}')\n",
        "  print(f'FINAL loss e:{epoch} = {1000*sum(tot_loss)/len(tot_loss):.5f}')\n",
        "  tot_loss=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 100/125 [03:34<00:53,  2.16s/itTrue]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss e:0 = 9500.44662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [04:28<00:00,  2.15s/itTrue]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FINAL loss e:0 = 9493.32954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWnqpBCV-TqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "7859a90e46cb4ba7bf8d204dae6f283f",
            "f3a828d26e5340fbbcd0a326276b0d19",
            "c7e4abe48d1d41829f8858f5e7749da0",
            "45a2ce75147346c591ff5d32b9cb2f35",
            "8d0298aeb61c413183b48408ed2d4659",
            "83b9a6ae36324bb9ba444217ec26298a",
            "b6c55ea1c0914d7f86d324ae47c38704",
            "09eab332aec14d8dbb2abb264a3b0bfc",
            "ba59fd811dbc4e308006f4ea640bed7b",
            "2bf910a9a8be4b3f959dcc14d30ed8e3",
            "64ddd51cef4740ddb0bafdc4b63417c8"
          ]
        },
        "outputId": "496a0575-3b18-48de-e3be-724b33326e3e"
      },
      "source": [
        "test_data = CNNDataset(mode='test', n_articles=1000)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "predicted, real = [], []\n",
        "\n",
        "model.eval()\n",
        "for batch in test_loader:\n",
        "\n",
        "  # generate hot-encoded summary using BART\n",
        "  summary_ids = model.generate(\n",
        "    batch['input_ids'].cuda(),\n",
        "    num_beams=4,\n",
        "    length_penalty=2.0,\n",
        "    no_repeat_ngram_size=3\n",
        "  )\n",
        "\n",
        "  # convert hot-encode into text\n",
        "  for sum_ids, tgt_ids in zip(summary_ids.cpu(), batch['decoder_input_ids']):\n",
        "    predicted_summary = test_data.tokenizer.decode(sum_ids, skip_special_tokens=True)\n",
        "    real_summary      = test_data.tokenizer.decode(tgt_ids, skip_special_tokens=True)\n",
        "\n",
        "    predicted.append(predicted_summary)\n",
        "    real.append(real_summary)\n",
        "\n",
        "rouge = load_metric('rouge')\n",
        "rouge.compute(predictions=predicted, references=real)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7859a90e46cb4ba7bf8d204dae6f283f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7859a90e46cb4ba7bf8d204dae6f283f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.2654222360550931, recall=0.41124030404098394, fmeasure=0.3163846305063779), mid=Score(precision=0.2728709818003307, recall=0.4211998226263163, fmeasure=0.3242209863000375), high=Score(precision=0.2796333294746038, recall=0.43031998802158794, fmeasure=0.33140917185333973)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.10641092098232009, recall=0.1667457545762694, fmeasure=0.12731087993487006), mid=Score(precision=0.1131658947297715, recall=0.17601675065083458, fmeasure=0.13469246299019255), high=Score(precision=0.12015714724502735, recall=0.18563892651058667, fmeasure=0.1424238803959585)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.18915586289745528, recall=0.29602743160142897, fmeasure=0.2263854091598036), mid=Score(precision=0.19613087356038905, recall=0.30535075925856336, fmeasure=0.23380691450104535), high=Score(precision=0.2034271587078356, recall=0.3150622048308093, fmeasure=0.2416123002715498)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.22227654184235474, recall=0.34450474644823537, fmeasure=0.2650742430395066), mid=Score(precision=0.2287277539325518, recall=0.35305773052802913, fmeasure=0.27177384510051417), high=Score(precision=0.2357666743211873, recall=0.36245942270367915, fmeasure=0.2794661559906377))}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.2654222360550931, recall=0.41124030404098394, fmeasure=0.3163846305063779), mid=Score(precision=0.2728709818003307, recall=0.4211998226263163, fmeasure=0.3242209863000375), high=Score(precision=0.2796333294746038, recall=0.43031998802158794, fmeasure=0.33140917185333973)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.10641092098232009, recall=0.1667457545762694, fmeasure=0.12731087993487006), mid=Score(precision=0.1131658947297715, recall=0.17601675065083458, fmeasure=0.13469246299019255), high=Score(precision=0.12015714724502735, recall=0.18563892651058667, fmeasure=0.1424238803959585)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.18915586289745528, recall=0.29602743160142897, fmeasure=0.2263854091598036), mid=Score(precision=0.19613087356038905, recall=0.30535075925856336, fmeasure=0.23380691450104535), high=Score(precision=0.2034271587078356, recall=0.3150622048308093, fmeasure=0.2416123002715498)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.22227654184235474, recall=0.34450474644823537, fmeasure=0.2650742430395066), mid=Score(precision=0.2287277539325518, recall=0.35305773052802913, fmeasure=0.27177384510051417), high=Score(precision=0.2357666743211873, recall=0.36245942270367915, fmeasure=0.2794661559906377))}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ILzEfbGuap",
        "outputId": "88c738ab-e945-4e34-dff4-04945ff32b0d"
      },
      "source": [
        "## print some examples\n",
        "\n",
        "for i in range(10,12):\n",
        "  print(f'REAL SUMMARY:\\n{real[i]}\\n\\nGENERATED:\\n{predicted[i]}\\n\\n', '_'*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REAL SUMMARY:\n",
            "U.S. military doesn't have further information to evaluate the Iraqi media reports.\n",
            "Al-Douri's body arrives in Baghdad where DNA samples are taken.\n",
            "Izzat Ibrahim al-Douri was the highest-ranking member of Iraqi President Saddam Hussein's regime to evade capture.\n",
            "\n",
            "GENERATED:\n",
            " Izzat Ibrahim al-Douri was the \"King of Clubs\" in a deck of playing cards used by U.S. troops to identify the most-wanted regime officials. He was killed in an operation by Iraqi security forces and Shia militia members in the Hamrin Mountains.\n",
            "\n",
            " ____________________________________________________________________________________________________\n",
            "REAL SUMMARY:\n",
            "Cory Booker: The unfortunate reality is that the United States leads the world in incarceration, not education.\n",
            "At the same time, we are losing the increasingly important race to educate our citizens.\n",
            "\n",
            "GENERATED:\n",
            " The United States is home to 25% of the world's prison population. Instead of empowering the next generation of American artists, scientists, engineers, inventors and entrepreneurs, our country has chosen to devote a massive amount of resources, time and energy to locking people up. By imprisoning individuals, we also burden families and breed economic inequality.\n",
            "\n",
            " ____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}