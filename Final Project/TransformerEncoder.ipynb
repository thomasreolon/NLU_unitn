{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycrGNFAFZ7Fz"
      },
      "source": [
        "# Encoder Text Summarizer\n",
        "\n",
        "Notebook for extracting summaries from the CNN articles dataset\n",
        "\n",
        "**Brief Summary:**\n",
        "\n",
        "- exprected running time 1h30 \n",
        "\n",
        "- the model uses an encoder to transform an hot-encoded text into an embedding, we then use these embeddings to say wether a token/sentence is important or not\n",
        "\n",
        "- we then create the summary by taking the 2 most important sentences from the article\n",
        "\n",
        "- results are decent altough distant from the state of the art: (selecting sentences randomly gives an Rouge-2 score of 0.04 while selecting the important ones gives a Rouge-2 score of 0.11)\n",
        "\n",
        "- problem: most of the ground truth highlights are abstractive (harder to train extractive). This makes the it harder to create a dataset that is capable of giving informations about which parts/tokens of the document are important. If we had extractive summaries we could have created the summaries by concatenating the most important tokens.\n",
        "\n",
        "**Notebook Sections:**\n",
        "\n",
        "- Installing libraries\n",
        "\n",
        "- CNN Dataset (from https://github.com/abisee/cnn-dailymail)\n",
        "\n",
        "- Neural Network (Transformer instead of RNN because RNN like LSTM are soooo long to training)\n",
        "\n",
        "- Instanciating Dataset/Model\n",
        "\n",
        "- Training (forward/backward prop)\n",
        "\n",
        "- Testing (generate summaries and get scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f88lrJleXif"
      },
      "source": [
        "## Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZEN69tFt2IK",
        "outputId": "ef00738f-1e09-4412-d89a-5f7ec517ae89"
      },
      "source": [
        "!pip install Rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ooRPRtztA"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from rouge import Rouge\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQudBoAk2lDZ"
      },
      "source": [
        "## CNN Dataset\n",
        "\n",
        "### Vocabolary to hot-encode the tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUwfLxhiGU-V"
      },
      "source": [
        "MAX_ARTICLE_LENGTH = 700         # default 400\n",
        "VOCAB_SIZE         = 42000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5uWGXX8raFm"
      },
      "source": [
        "# vocab will store these data (n_doc used for TF-IDF)\n",
        "class Word():\n",
        "  def __init__(self, idx):\n",
        "    self.idx : int  = idx                    # the hot-encoding\n",
        "    self.n_documents_with_word: int = 0      # how many articles it appeared in\n",
        "\n",
        "class Vocab():\n",
        "  def __init__(self, max_size=32000):\n",
        "    self.words = {}\n",
        "    self.idx   = 0\n",
        "    self.max_size = max_size\n",
        "    self.add_hotencode('[PAD]')\n",
        "    self.add_hotencode('[UNK]')\n",
        "    self.add_hotencode('.')\n",
        "\n",
        "  def get_hotencode(self, word): \n",
        "    if word in self.words:\n",
        "      return self.words[word].idx\n",
        "    else:\n",
        "      return self.words['[UNK]'].idx\n",
        "\n",
        "  def add_hotencode(self, word):\n",
        "    if word not in self.words and len(self.words)<self.max_size:\n",
        "      self.words[word] = Word(self.idx)  # add word to dict\n",
        "      self.idx += 1                      # next word will have a new code\n",
        "    elif word not in self.words and len(self.words)>=self.max_size:\n",
        "      return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP-HUN-S80et"
      },
      "source": [
        "## Class containing CNN Dataset\n",
        "\n",
        "- automated download of the files when called\n",
        "\n",
        "- returns:\n",
        "  - hotencoding of article (first 400 tokens)\n",
        "  - hotencoding of the summary\n",
        "  - tf-idf score for each token of the article\n",
        "  - importance score of each sentence of the article (a sentence is important if it contains a piece of the summary)\n",
        "  - importance score of each token of the article (a token is important if it is used in the summary)  ---> do not work too well: mostly abstractive summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6UdW9Oi84cz"
      },
      "source": [
        "class CNN_dailymail(Dataset):\n",
        "  def __init__(self, root='./dataset', article_length=400, n_articles = 100, offset=0, vocab_size=42000):\n",
        "    super().__init__()\n",
        "\n",
        "    # set up paths\n",
        "    self.root = root\n",
        "    self.article_length = article_length\n",
        "    self.n_articles = n_articles\n",
        "\n",
        "    # get data\n",
        "    if not os.path.exists(root):\n",
        "      self.download_dataset()\n",
        "    self.articles = os.listdir(f'{self.root}/cnn_stories_tokenized/')[offset:offset+n_articles]\n",
        "\n",
        "    # vocab\n",
        "    self.vocab = Vocab(max_size=vocab_size)\n",
        "    self.init_vocab()\n",
        "\n",
        "    # create tensors\n",
        "    self.data = self.make_dataset()\n",
        "\n",
        "  def init_vocab(self):\n",
        "    # track info for TF-IDF\n",
        "    for story in self.articles:\n",
        "      with open(f'{self.root}/cnn_stories_tokenized/{story}') as file_in:\n",
        "        text = file_in.read()\n",
        "      if not self.is_valid(text):\n",
        "        self.n_articles -= 1\n",
        "        continue\n",
        "      found = set()\n",
        "      text = text[19:].split('@highligh')\n",
        "      article =  text[0].split()\n",
        "\n",
        "      for token in article:\n",
        "        success = self.vocab.add_hotencode(token)\n",
        "        if success and token not in found:\n",
        "          self.vocab.words[token].n_documents_with_word += 1\n",
        "          found.add(token)\n",
        "\n",
        "  def get_best_match(self, summ, text, idx):\n",
        "    base = summ[idx:]\n",
        "    best, start, score = -1, 0, 0\n",
        "    for i, t in enumerate(text):\n",
        "      if len(base)>start and base[start] == t:\n",
        "        if start>=score:\n",
        "          best = i-start\n",
        "        start += 1\n",
        "      else:\n",
        "        start = 0\n",
        "    return best\n",
        "\n",
        "  def make_dataset(self):\n",
        "    \"\"\"read each file in the dataset folder (each file is an article) and extracts the data\"\"\"\n",
        "    data = []\n",
        "    self.texts = []\n",
        "\n",
        "    for story in self.articles:\n",
        "\n",
        "      # read article\n",
        "      with open(f'{self.root}/cnn_stories_tokenized/{story}') as file_in:\n",
        "        text = file_in.read()\n",
        "\n",
        "      # check goodness of article\n",
        "      if not self.is_valid(text): continue\n",
        "\n",
        "      # get tokens\n",
        "      text = text[19:].split('@highligh')\n",
        "      article =  text[0].split()[:self.article_length]\n",
        "      summary = [t.split() for t in text[1:]]\n",
        "      if len(article)<20: continue\n",
        "\n",
        "      # hotencode summary\n",
        "      y, i = torch.zeros(100, dtype=torch.long), 0\n",
        "      drop_sent = False\n",
        "      for sent in summary:\n",
        "        for j, token in enumerate(sent):\n",
        "          if i+j >= 100: break\n",
        "          idx    = self.vocab.get_hotencode(token)\n",
        "          if idx==1:\n",
        "            drop_sent = True\n",
        "          y[i+j] = idx\n",
        "        i += len(sent)\n",
        "      #if drop_sent: continue  ##### if unknown found ---> not extractive ---> DROP\n",
        "\n",
        "      # predictions\n",
        "      t_art = (' '.join(article)).split('. ')\n",
        "      t_sum = {' '.join(s) for s in summary}\n",
        "      self.texts.append((t_art, t_sum))    # save summary for testing\n",
        "      pred = torch.zeros(42)\n",
        "      for i, t in enumerate(t_art[:42]):\n",
        "        if any(t[:8] in t2 for t2 in t_sum) or any(t[-8:] in t2 for t2 in t_sum):\n",
        "          pred[i] = 1.\n",
        "\n",
        "      # count term frequencies (TF)\n",
        "      counts = defaultdict(lambda: 0)\n",
        "      for tok in article:\n",
        "        counts[tok] += 1\n",
        "\n",
        "      # hotencode article & TF-IDF\n",
        "      x = torch.zeros(self.article_length, dtype=torch.long)  # [PAD] has idx 0\n",
        "      tfidf = torch.zeros(self.article_length)\n",
        "      for i, token in enumerate(article):\n",
        "        # hot-encode\n",
        "        x[i]     = self.vocab.get_hotencode(token)\n",
        "\n",
        "        # TF-IDF\n",
        "        if token in self.vocab.words:\n",
        "          tfidf[i] = (counts[token] / self.article_length) * torch.log(torch.tensor(self.n_articles / self.vocab.words[token].n_documents_with_word))\n",
        "        \n",
        "      # word importance\n",
        "      word_importance = torch.zeros((2,self.article_length), dtype=torch.float)\n",
        "      full_summ='. '.join(list(t_sum)).split()\n",
        "      for i in range(len(full_summ)):\n",
        "        best_match = self.get_best_match(full_summ, article, i)\n",
        "        if best_match is None: continue\n",
        "        word_importance[0,best_match] = 1.\n",
        "        if i>0 and best_match>0 and full_summ[i-1] == article[best_match-1]:\n",
        "          if full_summ[i] != article[best_match]: print('AAAAAAAAAAA')\n",
        "          word_importance[1,best_match] = 1.\n",
        "\n",
        "      # add sample\n",
        "      data.append( (x,y,tfidf,pred, word_importance) )\n",
        "\n",
        "    return data\n",
        "\n",
        "  def get_sample(self, idx):\n",
        "    \"\"\"returns an encoded article and the text summary\"\"\"\n",
        "    x, _,_,_,_ = self.data[idx]\n",
        "    t_art, t_sum = self.texts[idx]\n",
        "    return x, t_art, t_sum\n",
        "\n",
        "  def is_valid(self, text):\n",
        "    valid = True\n",
        "    if len(text) < 100:           valid = False\n",
        "    elif '@highligh' not in text: valid = False\n",
        "\n",
        "    return valid\n",
        "\n",
        "\n",
        "  def download_dataset(self):\n",
        "    \"\"\"preprocessed dataset from https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail\"\"\"\n",
        "\n",
        "    os.system(f'mkdir {self.root}')\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id='0BzQ6rtO2VN95cmNuc2xwUS1wdEE',#'1C0MsFvsTdD-rCY6l4_kGsOEsKy46r_Yn',\n",
        "        dest_path=f\"{self.root}/data.zip\",\n",
        "        unzip=True)\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDVW9lhN21AL"
      },
      "source": [
        "## Neural Network Model\n",
        "\n",
        "todo:\n",
        "- add FF nn after CNN\n",
        "- improve ch2 w_importance as rogue-2 existence\n",
        "\n",
        "### positional encoder\n",
        "(from pytorch transformers tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SwtHUXrxXU1"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"add to embedding the positional encoding (sin/cos values)\"\"\"\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    \"\"\" x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\"\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLrS0w692-ep"
      },
      "source": [
        "## Transformer with Self-Attention\n",
        "\n",
        "**INPUT DOCUMENT**\n",
        "\n",
        "⬇\n",
        "\n",
        "2 Transformers Encoders (self-attention on whole document)\n",
        "\n",
        "⬇\n",
        "\n",
        "2 Transformers Encoders (self-attention on single sentences)\n",
        "\n",
        "⬇\n",
        "\n",
        "CNN\n",
        "\n",
        "⬇\n",
        "\n",
        "**SENTENCE SCORE** (as sum of the scores of the tokens of a sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wPdjYGXiuXo"
      },
      "source": [
        "class ModelSummarizer(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size=256, doc_len=400, dot_hotencode=2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dot_hotencode = dot_hotencode\n",
        "    self.d_model = emb_size\n",
        "\n",
        "    # hotencoding to vector\n",
        "    self.pos_encoder = PositionalEncoding(d_model=emb_size, dropout=0.4)\n",
        "    self.encoder = nn.Embedding(vocab_size, emb_size)\n",
        "    self.encoder.weight.data.uniform_(-.1, .1)\n",
        "\n",
        "    # transformer encoder 1\n",
        "    encoder_layers = nn.TransformerEncoderLayer(emb_size, 2, emb_size*2, .4, batch_first=True)\n",
        "    self.fullsent_encoder = nn.TransformerEncoder(encoder_layers, 2)\n",
        "\n",
        "    # transformer encoder 2\n",
        "    encoder_layers = nn.TransformerEncoderLayer(emb_size, 1, emb_size*2, .4, batch_first=True)\n",
        "    self.singlesent_encoder = nn.TransformerEncoder(encoder_layers, 2)\n",
        "\n",
        "    # sentence scorer\n",
        "    self.embedd_to_score = nn.Linear(emb_size,1)\n",
        "\n",
        "    # importance extraction\n",
        "    self.conv = nn.Conv1d(emb_size, 3, 3, padding=1)  # importance of  ch0:word    ch1:previous_and_current_word     ch2:sentence\n",
        "\n",
        "    # latent space vectorization\n",
        "    self.getlatent = nn.Sequential(\n",
        "        nn.Conv1d(emb_size, emb_size//2, 5, padding=1, stride=15, dilation=3),\n",
        "        nn.AdaptiveAvgPool1d(8),\n",
        "        nn.Flatten(),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8*emb_size//2, 512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, src):\n",
        "    # hotencoding to vector\n",
        "    x = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)                                 # (bs, doc_len, emb_size)\n",
        "    batch_size, sent_length, _ = x.shape\n",
        "\n",
        "    # attention on the whole sentence\n",
        "    x_mask = torch.zeros(sent_length, sent_length).to(src.device)\n",
        "    full_sent_emb = self.fullsent_encoder(x, x_mask)    \n",
        "    pred_tfidf = full_sent_emb[:,:,0]                       # (bs, doc_len, emb_size)\n",
        "\n",
        "    # attention on single sentences\n",
        "    src_mask = torch.ones(batch_size, sent_length, sent_length).to(src.device) * -1e20\n",
        "    all_dots = []\n",
        "    for i in range(batch_size):\n",
        "      dots = ((src[i] == 2).nonzero(as_tuple=True)[0])\n",
        "      all_dots.append(dots.tolist())\n",
        "      prev = 0\n",
        "      for d in dots:\n",
        "        d = d.item()\n",
        "        src_mask[i, prev:d+1, prev:d+1] = 0\n",
        "        prev = d\n",
        "      src_mask[i, prev:sent_length, prev:sent_length]\n",
        "    single_sent_emb = self.singlesent_encoder(full_sent_emb, src_mask) # (bs, doc_len, emb_size)\n",
        "\n",
        "    # summary\n",
        "    single_sent_emb = single_sent_emb.permute((0,2,1))   \n",
        "    words_importance = self.conv(single_sent_emb)            # (bs, doc_len, 3)   [ ch0:word    ch1:previous_and_current_word     ch2:sentence ]\n",
        "\n",
        "    # sentence importance\n",
        "    batch_docs = []\n",
        "    for i in range(batch_size):\n",
        "      doc, prev = [], 0\n",
        "      for d in all_dots[i]:\n",
        "        doc.append((words_importance[i, prev:d, 2]**2).sum().view(1))\n",
        "        prev = d\n",
        "      doc.append((words_importance[i, prev:sent_length, 2]**2).sum().view(1))\n",
        "      doc += [torch.tensor(0.).view(1).to(src.device)] * (42-len(doc))\n",
        "      doc = torch.cat(doc, dim=-1)[:42]\n",
        "      batch_docs.append(doc)\n",
        "    batch_docs = torch.stack(batch_docs)\n",
        "\n",
        "    # latent space of sentence\n",
        "    latent_space = self.getlatent(single_sent_emb)\n",
        "\n",
        "    # return stuff\n",
        "    results = {\n",
        "        'sent_importance':batch_docs,                   # a score that tells the importance of each sentence\n",
        "        'word_importance':words_importance[:,:2,:],     # score that thells if the word is in the summary\n",
        "        'tfidf':          pred_tfidf,                   # learn tf-idf as multitask\n",
        "        'latent':         latent_space                  # document to vector (summary similar to full doc)\n",
        "    }\n",
        "    return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "489pryA63m_D"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEAm36D33pH9"
      },
      "source": [
        "## Set Up network & stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au33rtrw1lVB",
        "outputId": "a37c81d4-88d1-4db2-e85d-f6381f3b0f53"
      },
      "source": [
        "# CPU or CUDA\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Dataset\n",
        "loader = torch.utils.data.DataLoader(CNN_dailymail(n_articles=200000, article_length=MAX_ARTICLE_LENGTH, vocab_size=VOCAB_SIZE), 8, pin_memory=True)\n",
        "\n",
        "# Neural Network\n",
        "net = ModelSummarizer(VOCAB_SIZE, doc_len=MAX_ARTICLE_LENGTH).to(device).train()\n",
        "optim = torch.optim.Adam(net.parameters(), lr=5e-3, weight_decay=1e-8)\n",
        "\n",
        "# trainable parameters\n",
        "sum(p.numel() for p in net.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 0BzQ6rtO2VN95cmNuc2xwUS1wdEE into ./dataset/data.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13551748"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUkay_Z3tXl"
      },
      "source": [
        "## Do Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhdOiTYCqQF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abe84ce-38b5-4576-c914-a17b9404b06f"
      },
      "source": [
        "for epoch in range(2):\n",
        "  tfloss, latloss, summloss = 0, 0, 0\n",
        "\n",
        "  for art, summ, tf_true, y_true, w_importance in tqdm(loader):\n",
        "    art, summ, tf_true, y_true, w_importance = art.to(device), summ.to(device), tf_true.to(device), y_true.to(device), w_importance.to(device)\n",
        "\n",
        "    # forward\n",
        "    doc_results = net(art)\n",
        "    sum_results = net(summ)\n",
        "\n",
        "    # losses\n",
        "    loss1 = nn.MSELoss()(doc_results['tfidf'], tf_true)\n",
        "    loss2 = torch.relu( 8 + nn.MSELoss()(doc_results['latent'], sum_results['latent']) - nn.MSELoss()(doc_results['latent'][1:], sum_results['latent'][:-1]) )\n",
        "    loss3 = nn.MSELoss()(doc_results['sent_importance'], y_true)\n",
        "    loss4 = nn.MSELoss()(doc_results['word_importance'], w_importance)\n",
        "    final_loss = loss1 + loss2 + 10*( loss3 + loss4 )\n",
        "\n",
        "    # backprop\n",
        "    final_loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # update stats\n",
        "    tfloss, latloss, summloss = tfloss+loss1.item(), latloss+loss2.item(), summloss+loss3.item()\n",
        "\n",
        "  print(f'losses for the epoch -->  tf-idf:{tfloss}, latentdistance:{latloss}, importantsents:{summloss}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11558/11558 [37:29<00:00,  5.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "losses for the epoch -->  tf-idf:226.17038132902235, latentdistance:96987.45580530167, importantsents:957.5618014745414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11558/11558 [37:34<00:00,  5.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "losses for the epoch -->  tf-idf:148.08792835334316, latentdistance:94606.98675918579, importantsents:890.7497558239847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr171j2L3vZ0"
      },
      "source": [
        "# Testing\n",
        "\n",
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGRs0r5ZQY3N"
      },
      "source": [
        "def get_summaries(dataset, printfirst=False):\n",
        "  outputs, abstracts = [], []\n",
        "\n",
        "  for idx in range(len(dataset)):\n",
        "    x, t_art, t_sum = dataset.get_sample(idx)\n",
        "\n",
        "    results = net(x.unsqueeze(0).to(device))\n",
        "    pred = results['sent_importance']\n",
        "    pred = pred.squeeze(0)[:len(t_art)]\n",
        "\n",
        "    t_pred = []\n",
        "    if len(t_art)>2:\n",
        "      _, top2 = pred.topk(k=3, dim=0)\n",
        "    else:\n",
        "      top2 = [0,1]\n",
        "\n",
        "    for i in top2:\n",
        "      if i >= len(t_art): i = 0\n",
        "      t_pred.append(t_art[i])\n",
        "\n",
        "    predicted = '. '.join(t_pred)\n",
        "    real      = '. '.join(list(t_sum))[2:]\n",
        "\n",
        "    if printfirst:\n",
        "      printfirst=False\n",
        "      print(f\"GROUND TRUTH SUMMARY\\n  {real}\\n{'_'*100}\\nEXTRACTED SUMMARY\\n  {predicted}\\n{'='*100}\\n\")\n",
        "\n",
        "    # stats\n",
        "    outputs.append(predicted)\n",
        "    abstracts.append(real)\n",
        "  \n",
        "  return outputs, abstracts\n",
        "\n",
        "def compute_accuracy(outputs, abstracts):\n",
        "  acc = Rouge().get_scores(outputs, abstracts, avg=True)\n",
        "  print(f\"\"\"\n",
        "Rouge-1: recall {acc['rouge-1']['r']}, precision {acc['rouge-1']['p']}, f1 {acc['rouge-1']['f']}\n",
        "Rouge-2: recall {acc['rouge-2']['r']}, precision {acc['rouge-2']['p']}, f1 {acc['rouge-2']['f']}\n",
        "Rouge-L: recall {acc['rouge-l']['r']}, precision {acc['rouge-l']['p']}, f1 {acc['rouge-l']['f']}\n",
        "        \"\"\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBzuyJjN3yyY"
      },
      "source": [
        "## Compute Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJxBBqvTktvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96c05aa-b16a-45a0-d31e-015686f1183d"
      },
      "source": [
        "net.eval()\n",
        "traindata = CNN_dailymail(n_articles=4000, offset=20000)\n",
        "testdata = CNN_dailymail(n_articles=4000, offset=50000)\n",
        " \n",
        "print('TRAINING SET')\n",
        "outputs, abstracts = get_summaries(traindata)\n",
        "compute_accuracy(outputs, abstracts)\n",
        "\n",
        "print('VALIDATION SET')\n",
        "outputs, abstracts = get_summaries(testdata, True)\n",
        "compute_accuracy(outputs, abstracts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING SET\n",
            "\n",
            "Rouge-1: recall 0.3825598650190738, precision 0.26851838415184, f1 0.3069110942724683\n",
            "Rouge-2: recall 0.13458653729294975, precision 0.08674773677889, f1 0.10185534606662996\n",
            "Rouge-L: recall 0.34906454924142116, precision 0.2451949880551469, f1 0.28012148252430363\n",
            "        \n",
            "VALIDATION SET\n",
            "GROUND TRUTH SUMMARY\n",
            "  They argue the U.S. should orient its foreign policy to favor pro-equality regimes. t Nations that wo n't commit to that goal do n't deserve American money or weapons , they say. t Authors : Terrorists in many nations oppose gender equality\n",
            "____________________________________________________________________________________________________\n",
            "EXTRACTED SUMMARY\n",
            "  Even terrorists have fears . The logic , for them , is simple . And the prospect of gender equality appears to rank high on their list of worst nightmares \n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "Rouge-1: recall 0.38579475555652465, precision 0.2683727565670014, f1 0.3085049602217897\n",
            "Rouge-2: recall 0.1367134627115836, precision 0.0870151193626517, f1 0.10294257074709502\n",
            "Rouge-L: recall 0.35086286346124224, precision 0.24432159299442205, f1 0.2806356715369073\n",
            "        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3nyqVPLk6u"
      },
      "source": [
        "### Comparison with Random selected sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3txhTNYKKXuk",
        "outputId": "2e3c90d5-0fc2-44ed-aa1f-b400f591c91d"
      },
      "source": [
        "def get_randomsummaries(dataset, printfirst=False):\n",
        "  outputs, abstracts = [], []\n",
        "\n",
        "  for idx in range(len(dataset)):\n",
        "    x, t_art, t_sum = dataset.get_sample(idx)\n",
        "    if len(t_art)==0: continue\n",
        "\n",
        "\n",
        "    t_pred = []\n",
        "    if len(t_art)>2:\n",
        "      pred = torch.rand(len(t_art))\n",
        "      pred = pred.squeeze(0)[:len(t_art)]\n",
        "      _, top2 = pred.topk(k=3, dim=0)\n",
        "      top2 = top2.tolist()\n",
        "    else:\n",
        "      top2 = [0]\n",
        "\n",
        "    for i in top2:\n",
        "      t_pred.append(t_art[i])\n",
        "\n",
        "    predicted = '. '.join(t_pred)\n",
        "    real      = '. '.join(list(t_sum))[2:]\n",
        "\n",
        "    if printfirst:\n",
        "      printfirst=False\n",
        "      print(f\"GROUND TRUTH SUMMARY\\n  {real}\\n{'_'*100}\\nEXTRACTED SUMMARY\\n  {predicted}\\n{'='*100}\\n\")\n",
        "\n",
        "    # stats\n",
        "    outputs.append(predicted)\n",
        "    abstracts.append(real)\n",
        "  \n",
        "  return outputs, abstracts\n",
        "\n",
        "print('TRAINING SET')\n",
        "outputs, abstracts = get_randomsummaries(traindata)\n",
        "compute_accuracy(outputs, abstracts)\n",
        "\n",
        "print('VALIDATION SET')\n",
        "outputs, abstracts = get_randomsummaries(testdata, True)\n",
        "compute_accuracy(outputs, abstracts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING SET\n",
            "\n",
            "Rouge-1: recall 0.2918446135875289, precision 0.22238655984579914, f1 0.24408942573128645\n",
            "Rouge-2: recall 0.0744033160152091, precision 0.05286653587265048, f1 0.05913813314411545\n",
            "Rouge-L: recall 0.2637194585694278, precision 0.2013533172360759, f1 0.22076545335946063\n",
            "        \n",
            "VALIDATION SET\n",
            "GROUND TRUTH SUMMARY\n",
            "  They argue the U.S. should orient its foreign policy to favor pro-equality regimes. t Nations that wo n't commit to that goal do n't deserve American money or weapons , they say. t Authors : Terrorists in many nations oppose gender equality\n",
            "____________________________________________________________________________________________________\n",
            "EXTRACTED SUMMARY\n",
            "  Even terrorists have fears . So , any successful strategy against these groups must put women 's rights at the front and center of policy planning . Educated women and girls would fundamentally challenge the power structure of organizations like ISIS \n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "Rouge-1: recall 0.29633085069116855, precision 0.22520064673591345, f1 0.24783062459490104\n",
            "Rouge-2: recall 0.07828215859220293, precision 0.05496822634161311, f1 0.06196466516745528\n",
            "Rouge-L: recall 0.26699091890355736, precision 0.20355745413070053, f1 0.2236119423527911\n",
            "        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0AlaJFniC6a"
      },
      "source": [
        "### Comparison with TF-IDF selected sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC_w92SGiEhE",
        "outputId": "0228e482-ea79-4739-ca35-0020c8f4e51d"
      },
      "source": [
        "def get_tfidfsummaries(dataset, printfirst=False):\n",
        "  outputs, abstracts = [], []\n",
        "\n",
        "  for idx in range(len(dataset)):\n",
        "    x, t_art, t_sum = dataset.get_sample(idx)\n",
        "    _,_,tf,_,_ = dataset[idx]\n",
        "\n",
        "    pred, prev = [], 0\n",
        "    for d in ((x == 2).nonzero(as_tuple=True)[0]):\n",
        "      pred.append(tf[prev:d].sum().view(1))\n",
        "      prev = d\n",
        "    pred.append(tf[prev:400].sum().view(1))\n",
        "    pred = torch.cat(pred, dim=-1)\n",
        "\n",
        "    #print(sum(  [1 for t in x if t==2]  ))\n",
        "    #print(pred, len(t_art))\n",
        "\n",
        "    t_pred = []\n",
        "    try:\n",
        "      _, top2 = pred.topk(k=3, dim=0)\n",
        "    except:\n",
        "      print('err')\n",
        "      top2 = [0,1]\n",
        "\n",
        "    for i in top2:\n",
        "      try:    t_pred.append(t_art[i])\n",
        "      except: pass\n",
        "\n",
        "    predicted = '. '.join(t_pred)\n",
        "    real      = '. '.join(list(t_sum))[2:]\n",
        "\n",
        "    if printfirst:\n",
        "      printfirst=False\n",
        "      print(f\"GROUND TRUTH SUMMARY\\n  {real}\\n{'_'*100}\\nEXTRACTED SUMMARY\\n  {predicted}\\n{'='*100}\\n\")\n",
        "\n",
        "    # stats\n",
        "    outputs.append(predicted)\n",
        "    abstracts.append(real)\n",
        "  \n",
        "  return outputs, abstracts\n",
        "\n",
        "print('TRAINING SET')\n",
        "outputs, abstracts = get_tfidfsummaries(traindata)\n",
        "compute_accuracy(outputs, abstracts)\n",
        "\n",
        "print('VALIDATION SET')\n",
        "outputs, abstracts = get_tfidfsummaries(testdata, True)\n",
        "compute_accuracy(outputs, abstracts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING SET\n",
            "err\n",
            "err\n",
            "err\n",
            "\n",
            "Rouge-1: recall 0.37931125622137246, precision 0.22912423199380835, f1 0.27754392479024775\n",
            "Rouge-2: recall 0.11962545575379759, precision 0.06435749945905123, f1 0.080655679990919\n",
            "Rouge-L: recall 0.34168043309850366, precision 0.20678630036821835, f1 0.25024388981194634\n",
            "        \n",
            "VALIDATION SET\n",
            "GROUND TRUTH SUMMARY\n",
            "  They argue the U.S. should orient its foreign policy to favor pro-equality regimes. t Nations that wo n't commit to that goal do n't deserve American money or weapons , they say. t Authors : Terrorists in many nations oppose gender equality\n",
            "____________________________________________________________________________________________________\n",
            "EXTRACTED SUMMARY\n",
            "  investment . Does any group or state that refuses to commit to working toward gender equality merit our money , weapons or political capital ? Any entity that refuses to treat at least half of its population as equal to the other can not be expected to protect minorities and promote tolerance . `` There is no doubt that targeting of women is the core element -- not a byproduct -- of the ideologies espoused by these groups , '' Sanam Naraghi-Anderlini , co-founder of the International Civil Society Action Network , told us \n",
            "====================================================================================================\n",
            "\n",
            "err\n",
            "err\n",
            "\n",
            "Rouge-1: recall 0.3787037743091421, precision 0.2292606987881041, f1 0.27739063736425024\n",
            "Rouge-2: recall 0.12113726352842728, precision 0.06529267058689554, f1 0.08168112185544735\n",
            "Rouge-L: recall 0.34143685843589133, precision 0.20706363087659063, f1 0.25028287337463456\n",
            "        \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}